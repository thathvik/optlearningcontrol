{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is notebook file is the Solution for Exercise 2 from the [exercise series 3](https://github.com/righetti/optlearningcontrol/tree/master/Spring2020/series3) for the course Optimal and Learning Control (ME-GY 7973), as solved by Tarun Thathvik Paladugu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_policy(J, possible_moves):\n",
    "    A = np.zeros([J.size,J.size])\n",
    "    for i in range(J.size):\n",
    "        low = np.inf\n",
    "        for j in range(possible_moves[i].size):\n",
    "            if possible_moves[i][j] == 1:\n",
    "                if low > J[j]:\n",
    "                    low = J[j]\n",
    "                    A[i] = np.zeros([A[i].size])\n",
    "                    A[i,j] = 1\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining all the unchanged variables for question 1 and 2\n",
    "g = np.array(([0,0,1,0,-1,0,1,0,10,0,10,1,0,1,0,0,10,-1]))\n",
    "alpha = 0.99\n",
    "possible_moves = np.zeros([18,18])\n",
    "\n",
    "possible_moves[0,0] = 1\n",
    "possible_moves[0,1] = 1\n",
    "possible_moves[0,5] = 1\n",
    "possible_moves[1,0] = 1\n",
    "possible_moves[1,1] = 1\n",
    "possible_moves[1,2] = 1\n",
    "possible_moves[2,1] = 1\n",
    "possible_moves[2,2] = 1\n",
    "possible_moves[2,3] = 1\n",
    "possible_moves[2,6] = 1\n",
    "possible_moves[3,2] = 1\n",
    "possible_moves[3,3] = 1\n",
    "possible_moves[3,4] = 1\n",
    "possible_moves[3,7] = 1\n",
    "possible_moves[4,3] = 1\n",
    "possible_moves[4,4] = 1\n",
    "possible_moves[4,8] = 1\n",
    "possible_moves[5,0] = 1\n",
    "possible_moves[5,5] = 1\n",
    "possible_moves[5,9] = 1\n",
    "possible_moves[6,2] = 1\n",
    "possible_moves[6,6] = 1\n",
    "possible_moves[6,7] = 1\n",
    "possible_moves[7,3] = 1\n",
    "possible_moves[7,6] = 1\n",
    "possible_moves[7,7] = 1\n",
    "possible_moves[7,8] = 1\n",
    "possible_moves[7,11] = 1\n",
    "possible_moves[8,4] = 1\n",
    "possible_moves[8,7] = 1\n",
    "possible_moves[8,8] = 1\n",
    "possible_moves[8,12] = 1\n",
    "possible_moves[9,5] = 1\n",
    "possible_moves[9,9] = 1\n",
    "possible_moves[9,10] = 1\n",
    "possible_moves[9,13] = 1\n",
    "possible_moves[10,9] = 1\n",
    "possible_moves[10,10] = 1\n",
    "possible_moves[10,14] = 1\n",
    "possible_moves[11,7] = 1\n",
    "possible_moves[11,11] = 1\n",
    "possible_moves[11,12] = 1\n",
    "possible_moves[11,16] = 1\n",
    "possible_moves[12,8] = 1\n",
    "possible_moves[12,11] = 1\n",
    "possible_moves[12,12] = 1\n",
    "possible_moves[12,17] = 1\n",
    "possible_moves[13,9] = 1\n",
    "possible_moves[13,13] = 1\n",
    "possible_moves[13,14] = 1\n",
    "possible_moves[14,10] = 1\n",
    "possible_moves[14,13] = 1\n",
    "possible_moves[14,14] = 1\n",
    "possible_moves[14,15] = 1\n",
    "possible_moves[15,14] = 1\n",
    "possible_moves[15,15] = 1\n",
    "possible_moves[15,16] = 1\n",
    "possible_moves[16,11] = 1\n",
    "possible_moves[16,15] = 1\n",
    "possible_moves[16,16] = 1\n",
    "possible_moves[16,17] = 1\n",
    "possible_moves[17,12] = 1\n",
    "possible_moves[17,16] = 1\n",
    "possible_moves[17,17] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "### Part a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Iterations until convergance =  1376\n",
      "The final cost = \n",
      "[-95.07940237 -96.03980137 -97.00990137 -98.99990137 -99.99990137\n",
      " -94.12860736 -96.02980137 -98.00990137 -88.99990137 -93.1873203\n",
      " -82.25544611 -97.00990137 -98.99990137 -91.25544611 -90.34289066\n",
      " -89.43946077 -88.99990137 -99.99990137]\n"
     ]
    }
   ],
   "source": [
    "#Value Iteration\n",
    "\n",
    "iteration = 1\n",
    "change = 10**(-6)\n",
    "\n",
    "#defining the initial J\n",
    "J = np.full(g.shape, 0)\n",
    "\n",
    "\n",
    "while(1):\n",
    "    #while\n",
    "    next_step = np.full(J.shape, np.inf)\n",
    "    for i in range(possible_moves[0].size):\n",
    "        for j in range(possible_moves[i].size):\n",
    "            if possible_moves[i][j] == 1:\n",
    "                if next_step[i] > J[j]:\n",
    "                    next_step[i] = J[j]\n",
    "    J_old = J\n",
    "    J = g + alpha*next_step\n",
    "    if all(abs(J - J_old) < change):\n",
    "        break\n",
    "    iteration += 1\n",
    "    \n",
    "print(\"The number of Iterations until convergance = \", iteration)\n",
    "print(\"The final cost = \")\n",
    "J_a = np.full(J.size, J)\n",
    "print(J_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "### Part b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Iterations until convergance =  8\n",
      "The final cost = \n",
      "[ -95.079501    -96.0399      -97.01        -99.         -100.\n",
      "  -94.12870599  -96.0299      -98.01        -89.          -93.18741893\n",
      "  -82.25554474  -97.01        -99.          -91.25554474  -90.34298929\n",
      "  -89.4395594   -89.         -100.        ]\n",
      "\n",
      "The final Policy after Policy Iteration: \n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Initiating an arbitrary A that does not move\n",
    "A_old = np.eye(18)\n",
    "\n",
    "#Initiating variable to keep count of Iterations\n",
    "iteration = 1\n",
    "\n",
    "#change = 0.000001  #Used when checking for the convergance of J\n",
    "while(1):\n",
    "    J = np.linalg.solve(np.eye(18) - alpha * A_old, g)\n",
    "    A = find_policy(J,possible_moves)     #Finding the new A from the new J\n",
    "    #if all(abs(J - J_old) <= change):\n",
    "    if (A == A_old).all():\n",
    "        break\n",
    "    #J_old = J\n",
    "    A_old = np.full(A.shape, A)\n",
    "    iteration += 1\n",
    "\n",
    "print(\"The number of Iterations until convergance = \", iteration)\n",
    "print(\"The final cost = \")\n",
    "J_b = np.full(J.size, J)\n",
    "print(J_b)\n",
    "\n",
    "print()\n",
    "print(\"The final Policy after Policy Iteration: \")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c)\n",
    "\n",
    "Solving Linear Algebra and the large size of the A matrix might make Policy Iteration slightly more computationally intensive in each step, but the total number of iterations taken to converge to a desired Policy is much greater for Value Iteration which makes the overall time taken much higher for the value iteration (when run, it was found that the time taken to run the Policy Iteration is .63s while the time taken to run the value Iteration is .32s). The policies in Policy Iteration converges at the 8th iteration, while it takes 1376 iterations for the cost function to converge through the value iteration converges. From it is clear that 1376 iterations of value iteration took only twice as long as the 8 iterations of the Policy Iteration algorithm. Therefore, the Policy Iteration Algorithm is computationally intensive but gives faster results, while the Value Iteration is relatively very light for computation, yet slow to converge. It is a faor comparison since the initail conditions for both the value iteration and policy iteration are the same (Zero Value inital condition is the same as the policy that does not move). \n",
    "\n",
    "In other words, the quick convergence of the Policy Iteration more than compensates for the higher complexity due to linear algebra. And due to the slow convergence of the Value Iteration, it takes much longer to compute although the complexity of the algorithm is very low.\n",
    "\n",
    "<p> However, despite the large differences in the number of iterations and time consumption, the J from Value iteration and J$_{\\mu}$ from Policy iteration are almost equal. And therefore the policies are both equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J from Value Iteration: \n",
      "[-95.07940237 -96.03980137 -97.00990137 -98.99990137 -99.99990137\n",
      " -94.12860736 -96.02980137 -98.00990137 -88.99990137 -93.1873203\n",
      " -82.25544611 -97.00990137 -98.99990137 -91.25544611 -90.34289066\n",
      " -89.43946077 -88.99990137 -99.99990137]\n",
      "J from Policy Iteration: \n",
      "[ -95.079501    -96.0399      -97.01        -99.         -100.\n",
      "  -94.12870599  -96.0299      -98.01        -89.          -93.18741893\n",
      "  -82.25554474  -97.01        -99.          -91.25554474  -90.34298929\n",
      "  -89.4395594   -89.         -100.        ]\n",
      "\n",
      "All the values of both the policies are equall :  True\n"
     ]
    }
   ],
   "source": [
    "print(\"J from Value Iteration: \")\n",
    "print(J_a)\n",
    "print(\"J from Policy Iteration: \")\n",
    "print(J_b)\n",
    "\n",
    "print()\n",
    "print(\"All the values of both the policies are equall : \", (find_policy(J_a, possible_moves)==find_policy(J_b, possible_moves)).all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
